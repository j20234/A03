# é«˜çº§æ€§èƒ½ä¼˜åŒ–æŒ‡å—

## æ¦‚è¿°

æœ¬æ–‡æ¡£æä¾›äº†é‡åŒ–äº¤æ˜“æ¡†æ¶çš„é«˜çº§æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯ï¼ŒåŒ…æ‹¬Numba JITç¼–è¯‘ã€å¹¶è¡Œå¤„ç†ã€å†…å­˜ä¼˜åŒ–ç­‰ã€‚

---

## ğŸš€ ç¬¬ä¸€å±‚ä¼˜åŒ–ï¼šåŸºç¡€ä¼˜åŒ–ï¼ˆå·²å®Œæˆï¼‰

è¿™äº›ä¼˜åŒ–å·²ç»åœ¨ä¸»ä»£ç ä¸­å®ç°ï¼š

### âœ… æ•°æ®è®¿é—®ä¼˜åŒ–
- ä½¿ç”¨ `iloc[]` æ›¿ä»£ `iterrows()`
- ä½¿ç”¨ numpy æ•°ç»„æ›¿ä»£ pandas Series
- é¢„å…ˆæå–å¸¸ç”¨åˆ—

**æ€§èƒ½æå‡**: 10-50å€

### âœ… å†…å­˜ç®¡ç†
- ä½¿ç”¨ float32 æ›¿ä»£ float64
- æ»‘åŠ¨çª—å£é™åˆ¶å†å²æ•°æ®
- åŠæ—¶æ¸…ç†ä¸ç”¨çš„å¯¹è±¡

**å†…å­˜èŠ‚çœ**: 50-80%

### âœ… å‘é‡åŒ–è®¡ç®—
- numpy å‘é‡åŒ–æ“ä½œ
- é¿å…å¾ªç¯ä¸­çš„ DataFrame æ“ä½œ
- æ‰¹é‡å¤„ç†æ•°æ®

**æ€§èƒ½æå‡**: 5-20å€

---

## âš¡ ç¬¬äºŒå±‚ä¼˜åŒ–ï¼šNumba JITç¼–è¯‘

### ä»€ä¹ˆæ˜¯Numbaï¼Ÿ

Numba æ˜¯ä¸€ä¸ªJITï¼ˆå³æ—¶ç¼–è¯‘ï¼‰ç¼–è¯‘å™¨ï¼Œå¯ä»¥å°†Pythonä»£ç ç¼–è¯‘æˆæœºå™¨ç ï¼Œæä¾›æ¥è¿‘Cçš„æ€§èƒ½ã€‚

### å®‰è£…
```bash
pip install numba
```

### ä½¿ç”¨ç¤ºä¾‹

#### 1. åŸºç¡€JITä¼˜åŒ–

```python
from numba import jit
import numpy as np

# åŸå§‹Pythonå‡½æ•°
def slow_function(arr):
    result = 0
    for i in range(len(arr)):
        result += arr[i] * 2
    return result

# NumbaåŠ é€Ÿç‰ˆæœ¬
@jit(nopython=True)
def fast_function(arr):
    result = 0
    for i in range(len(arr)):
        result += arr[i] * 2
    return result

# æ€§èƒ½æµ‹è¯•
data = np.random.randn(1000000)
%timeit slow_function(data)  # ~200ms
%timeit fast_function(data)  # ~2ms (100x faster!)
```

#### 2. æŠ€æœ¯æŒ‡æ ‡åŠ é€Ÿ

ä½¿ç”¨ `numba_optimizations.py` ä¸­çš„ä¼˜åŒ–æŒ‡æ ‡ï¼š

```python
from numba_optimizations import NumbaOptimizedIndicators

# ç”Ÿæˆä»·æ ¼æ•°æ®
prices = np.random.randn(100000).cumsum() + 100

# å¿«é€ŸRSIè®¡ç®—
rsi = NumbaOptimizedIndicators.rsi(prices, period=14)

# å¿«é€ŸMACDè®¡ç®—
macd_line, signal_line, histogram = NumbaOptimizedIndicators.macd(prices)

# å¿«é€Ÿå¸ƒæ—å¸¦
upper, middle, lower = NumbaOptimizedIndicators.bollinger_bands(prices)
```

**æ€§èƒ½æå‡**: 50-100å€

#### 3. å¹¶è¡Œè®¡ç®—

```python
from numba import jit, prange

@jit(nopython=True, parallel=True)
def parallel_computation(data_matrix):
    n, m = data_matrix.shape
    result = np.zeros(n)
    
    for i in prange(n):  # å¹¶è¡Œå¾ªç¯
        result[i] = np.sum(data_matrix[i])
    
    return result
```

**æ€§èƒ½æå‡**: é¢å¤–2-8å€ï¼ˆå–å†³äºCPUæ ¸å¿ƒæ•°ï¼‰

---

## ğŸ’¾ ç¬¬ä¸‰å±‚ä¼˜åŒ–ï¼šæ™ºèƒ½ç¼“å­˜

### ä½¿ç”¨æ€§èƒ½å·¥å…·ä¸­çš„ç¼“å­˜ç®¡ç†å™¨

```python
from performance_utils import CacheManager

# åˆ›å»ºç¼“å­˜ç®¡ç†å™¨
cache = CacheManager(max_cache_size=100)

# åœ¨ç­–ç•¥ä¸­ä½¿ç”¨
class OptimizedStrategy:
    def __init__(self):
        self.cache = CacheManager()
    
    def get_factor_value(self, symbol, date):
        # å°è¯•ä»ç¼“å­˜è·å–
        cache_key = f"{symbol}_{date}"
        cached_value = self.cache.get(cache_key)
        
        if cached_value is not None:
            return cached_value
        
        # è®¡ç®—å› å­å€¼
        value = self._calculate_factor(symbol, date)
        
        # å­˜å…¥ç¼“å­˜
        self.cache.set(cache_key, value)
        
        return value
```

**æ€§èƒ½æå‡**: é¿å…é‡å¤è®¡ç®—ï¼Œ2-10å€

### ä½¿ç”¨functools.lru_cache

```python
from functools import lru_cache

@lru_cache(maxsize=128)
def expensive_calculation(param1, param2):
    # æ˜‚è´µçš„è®¡ç®—
    return result

# ç¬¬ä¸€æ¬¡è°ƒç”¨ä¼šè®¡ç®—
result1 = expensive_calculation(10, 20)  # æ…¢

# åç»­ç›¸åŒå‚æ•°çš„è°ƒç”¨ç›´æ¥è¿”å›ç¼“å­˜
result2 = expensive_calculation(10, 20)  # æå¿«ï¼
```

---

## ğŸ”„ ç¬¬å››å±‚ä¼˜åŒ–ï¼šå¹¶è¡Œå¤„ç†

### 1. å¤šè¿›ç¨‹å¤„ç†å¤šä¸ªè‚¡ç¥¨

```python
from multiprocessing import Pool
import pandas as pd

def process_symbol(symbol):
    # åŠ è½½æ•°æ®
    data = load_data(symbol)
    
    # è¿è¡Œç­–ç•¥
    results = run_strategy(data)
    
    return symbol, results

# å¹¶è¡Œå¤„ç†
symbols = ['AAPL', 'GOOGL', 'MSFT', 'AMZN']
with Pool(processes=4) as pool:
    results = pool.map(process_symbol, symbols)
```

**æ€§èƒ½æå‡**: æ¥è¿‘çº¿æ€§ï¼ˆ4æ ¸çº¦4å€ï¼‰

### 2. ä½¿ç”¨joblib

```python
from joblib import Parallel, delayed

def process_date(date):
    # å¤„ç†æŸä¸€å¤©çš„æ•°æ®
    return compute_signals(date)

# å¹¶è¡Œå¤„ç†æ‰€æœ‰æ—¥æœŸ
dates = pd.date_range('2020-01-01', '2023-12-31')
results = Parallel(n_jobs=-1)(
    delayed(process_date)(date) for date in dates
)
```

---

## ğŸ“Š ç¬¬äº”å±‚ä¼˜åŒ–ï¼šæ•°æ®å­˜å‚¨ä¼˜åŒ–

### 1. ä½¿ç”¨Parquetæ ¼å¼

Parquetæ˜¯åˆ—å¼å­˜å‚¨æ ¼å¼ï¼Œæ¯”CSVå¿«5-10å€ï¼Œä¸”æ–‡ä»¶æ›´å°ã€‚

```python
# ä¿å­˜ä¸ºParquet
df.to_parquet('data.parquet', compression='snappy')

# è¯»å–Parquet
df = pd.read_parquet('data.parquet')

# æ€§èƒ½å¯¹æ¯”
# CSV:     1.2s, 100MB
# Parquet: 0.15s, 30MB (8x faster, 70% smaller)
```

### 2. ä½¿ç”¨HDF5

```python
# å†™å…¥HDF5
store = pd.HDFStore('data.h5')
store['prices'] = df
store.close()

# è¯»å–HDF5
df = pd.read_hdf('data.h5', 'prices')

# éƒ¨åˆ†è¯»å–ï¼ˆæŒ‰æ¡ä»¶ï¼‰
df_subset = pd.read_hdf('data.h5', 'prices', 
                        where='date > "2020-01-01"')
```

### 3. ä½¿ç”¨æ•°æ®åº“

```python
from sqlalchemy import create_engine

# åˆ›å»ºæ•°æ®åº“è¿æ¥
engine = create_engine('sqlite:///market_data.db')

# å†™å…¥æ•°æ®åº“
df.to_sql('prices', engine, if_exists='append', index=False)

# è¯»å–æ•°æ®åº“ï¼ˆä½¿ç”¨SQLä¼˜åŒ–ï¼‰
query = """
SELECT * FROM prices 
WHERE symbol = 'AAPL' 
  AND date > '2020-01-01'
ORDER BY date
"""
df = pd.read_sql(query, engine)
```

---

## ğŸ§® ç¬¬å…­å±‚ä¼˜åŒ–ï¼šç®—æ³•ä¼˜åŒ–

### 1. å¢é‡è®¡ç®—

ä¸è¦æ¯æ¬¡éƒ½é‡æ–°è®¡ç®—æ•´ä¸ªå†å²ï¼Œè€Œæ˜¯å¢é‡æ›´æ–°ï¼š

```python
class IncrementalMovingAverage:
    def __init__(self, window):
        self.window = window
        self.values = []
        self.sum = 0
        self.ma = None
    
    def update(self, new_value):
        self.values.append(new_value)
        self.sum += new_value
        
        if len(self.values) > self.window:
            old_value = self.values.pop(0)
            self.sum -= old_value
        
        if len(self.values) == self.window:
            self.ma = self.sum / self.window
        
        return self.ma

# ä½¿ç”¨
ma = IncrementalMovingAverage(20)
for price in prices:
    current_ma = ma.update(price)
```

**æ€§èƒ½æå‡**: O(n) å˜ä¸º O(1)

### 2. ä½¿ç”¨æ›´å¿«çš„ç®—æ³•

ä¾‹å¦‚ï¼Œè®¡ç®—æ»šåŠ¨æ ‡å‡†å·®ï¼š

```python
# æ…¢ï¼šæ¯æ¬¡é‡æ–°è®¡ç®—
def slow_rolling_std(data, window):
    result = []
    for i in range(window, len(data)):
        result.append(np.std(data[i-window:i]))
    return result

# å¿«ï¼šä½¿ç”¨Welfordç®—æ³•å¢é‡è®¡ç®—
class WelfordStd:
    def __init__(self, window):
        self.window = window
        self.values = []
        self.mean = 0
        self.m2 = 0
    
    def update(self, value):
        self.values.append(value)
        
        if len(self.values) > self.window:
            old_value = self.values.pop(0)
            # å¢é‡æ›´æ–°meanå’Œm2
            ...
        
        return np.sqrt(self.m2 / self.window)
```

---

## ğŸ¯ æ€§èƒ½åˆ†æå·¥å…·ä½¿ç”¨

### 1. ä½¿ç”¨æ€§èƒ½åˆ†æå™¨

```python
from performance_utils import PerformanceProfiler

profiler = PerformanceProfiler()

@profiler.timer
def my_strategy_function():
    # ç­–ç•¥é€»è¾‘
    pass

@profiler.timer
def calculate_factors():
    # å› å­è®¡ç®—
    pass

# è¿è¡Œç­–ç•¥
run_backtest()

# æŸ¥çœ‹æ€§èƒ½æŠ¥å‘Š
profiler.print_report()
```

### 2. è¿è¡ŒåŸºå‡†æµ‹è¯•

```bash
# å¿«é€Ÿæµ‹è¯•
python benchmark.py quick

# å®Œæ•´æµ‹è¯•
python benchmark.py

# è¾“å‡ºç¤ºä¾‹ï¼š
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æµ‹è¯• 1: æ•°æ®è¿­ä»£æ€§èƒ½å¯¹æ¯”
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# iterrows:    245.32ms (åŸºå‡†)
# itertuples:   45.21ms (5.4x æ›´å¿«)
# iloc:         38.15ms (6.4x æ›´å¿«)
# values:        2.31ms (106.2x æ›´å¿«) â­æ¨è
```

### 3. å†…å­˜åˆ†æ

```python
from performance_utils import DataFrameOptimizer

# åˆ†æDataFrameå†…å­˜ä½¿ç”¨
report = DataFrameOptimizer.memory_usage_report(df)
print(report)

# ä¼˜åŒ–DataFrame
optimized_df = DataFrameOptimizer.optimize_dtypes(df)

# æŸ¥çœ‹èŠ‚çœçš„å†…å­˜
print(f"èŠ‚çœå†…å­˜: {(1 - report_after/report_before)*100:.1f}%")
```

---

## ğŸ“ˆ å®æˆ˜æ¡ˆä¾‹

### æ¡ˆä¾‹1ï¼šä¼˜åŒ–10ä¸‡æ¡æ•°æ®çš„å›æµ‹

**ä¼˜åŒ–å‰**:
- æ—¶é—´: 120ç§’
- å†…å­˜: 3.2GB
- CPU: å•æ ¸100%

**åº”ç”¨ä¼˜åŒ–**:
1. ä½¿ç”¨ilocæ›¿ä»£iterrows
2. ä½¿ç”¨numpyå‘é‡åŒ–
3. å¯ç”¨Numba JIT
4. ä½¿ç”¨float32
5. æ»‘åŠ¨çª—å£é™åˆ¶

**ä¼˜åŒ–å**:
- æ—¶é—´: 3.2ç§’ (37x faster)
- å†…å­˜: 450MB (86% reduction)
- CPU: å¤šæ ¸ä½¿ç”¨

### æ¡ˆä¾‹2ï¼šå¤šè‚¡ç¥¨ç»„åˆä¼˜åŒ–

**åŸå§‹ä»£ç **:
```python
# é¡ºåºå¤„ç†
results = {}
for symbol in symbols:  # 100ä¸ªè‚¡ç¥¨
    results[symbol] = process_symbol(symbol)  # æ¯ä¸ª2ç§’
# æ€»æ—¶é—´: 200ç§’
```

**ä¼˜åŒ–ä»£ç **:
```python
# å¹¶è¡Œå¤„ç†
from multiprocessing import Pool
with Pool(8) as pool:  # 8æ ¸
    results = pool.map(process_symbol, symbols)
# æ€»æ—¶é—´: 28ç§’ (7x faster)
```

---

## ğŸ” æ€§èƒ½è¯Šæ–­æ¸…å•

ä½¿ç”¨æ­¤æ¸…å•è¯Šæ–­æ€§èƒ½é—®é¢˜ï¼š

- [ ] æ˜¯å¦ä½¿ç”¨äº†iterrows()ï¼Ÿ â†’ æ”¹ç”¨ilocæˆ–values
- [ ] æ˜¯å¦åœ¨å¾ªç¯ä¸­ä½¿ç”¨pd.concat()ï¼Ÿ â†’ æ”¹ç”¨åˆ—è¡¨ç¼“å­˜
- [ ] æ˜¯å¦ä½¿ç”¨äº†float64ï¼Ÿ â†’ æ”¹ç”¨float32
- [ ] æ˜¯å¦æœ‰å¤§é‡é‡å¤è®¡ç®—ï¼Ÿ â†’ æ·»åŠ ç¼“å­˜
- [ ] æ˜¯å¦å¯ä»¥å‘é‡åŒ–ï¼Ÿ â†’ ä½¿ç”¨numpy
- [ ] æ˜¯å¦å¯ä»¥JITç¼–è¯‘ï¼Ÿ â†’ ä½¿ç”¨Numba
- [ ] æ˜¯å¦å¯ä»¥å¹¶è¡Œï¼Ÿ â†’ ä½¿ç”¨multiprocessing
- [ ] DataFrameæ˜¯å¦è¿‡å¤§ï¼Ÿ â†’ ä¼˜åŒ–æ•°æ®ç±»å‹æˆ–åˆ†å—
- [ ] æ˜¯å¦æœ‰å†…å­˜æ³„æ¼ï¼Ÿ â†’ æ£€æŸ¥æ»‘åŠ¨çª—å£å’Œæ¸…ç†

---

## ğŸ“ æœ€ä½³å®è·µæ€»ç»“

### æ€§èƒ½ä¼˜åŒ–ä¼˜å…ˆçº§

1. **ç®—æ³•ä¼˜åŒ–** (æœ€é‡è¦)
   - é€‰æ‹©æ­£ç¡®çš„ç®—æ³•
   - é¿å…ä¸å¿…è¦çš„è®¡ç®—

2. **æ•°æ®ç»“æ„ä¼˜åŒ–**
   - ä½¿ç”¨åˆé€‚çš„æ•°æ®ç±»å‹
   - é¿å…é¢‘ç¹çš„ç±»å‹è½¬æ¢

3. **å‘é‡åŒ–**
   - ä½¿ç”¨numpyè€Œä¸æ˜¯å¾ªç¯
   - æ‰¹é‡å¤„ç†æ•°æ®

4. **JITç¼–è¯‘**
   - å¯¹çƒ­ç‚¹ä»£ç ä½¿ç”¨Numba
   - ç¼–è¯‘å¯†é›†è®¡ç®—å‡½æ•°

5. **å¹¶è¡Œå¤„ç†**
   - å¤„ç†ç‹¬ç«‹ä»»åŠ¡æ—¶ä½¿ç”¨å¤šè¿›ç¨‹
   - åˆç†åˆ©ç”¨CPUèµ„æº

6. **ç¼“å­˜å’Œå¤ç”¨**
   - ç¼“å­˜é‡å¤è®¡ç®—ç»“æœ
   - å¢é‡è®¡ç®—è€Œéå…¨é‡

### é¿å…å¸¸è§é™·é˜±

âŒ **ä¸è¦åšçš„äº‹**:
- ä¸è¦åœ¨å¾ªç¯ä¸­åˆ›å»ºDataFrame
- ä¸è¦ä½¿ç”¨iterrows()
- ä¸è¦åœ¨å¾ªç¯ä¸­pd.concat()
- ä¸è¦å¿½ç•¥å†…å­˜ç®¡ç†
- ä¸è¦è¿‡æ—©ä¼˜åŒ–ï¼ˆå…ˆç¡®ä¿æ­£ç¡®æ€§ï¼‰

âœ… **åº”è¯¥åšçš„äº‹**:
- ä½¿ç”¨profileræ‰¾å‡ºç“¶é¢ˆ
- å…ˆä¼˜åŒ–ç®—æ³•ï¼Œå†ä¼˜åŒ–ä»£ç 
- ä½¿ç”¨åŸºå‡†æµ‹è¯•éªŒè¯ä¼˜åŒ–æ•ˆæœ
- ä¿æŒä»£ç å¯è¯»æ€§
- æ–‡æ¡£åŒ–æ€§èƒ½å…³é”®éƒ¨åˆ†

---

## ğŸ“ è¿›é˜¶ä¸»é¢˜

### 1. GPUåŠ é€Ÿï¼ˆé«˜çº§ï¼‰

ä½¿ç”¨CuPyï¼ˆCUDAï¼‰æˆ–PyOpenCLè¿›è¡ŒGPUè®¡ç®—ï¼š

```python
import cupy as cp  # éœ€è¦NVIDIA GPU

# CPUç‰ˆæœ¬
data_cpu = np.random.randn(10000, 10000)
result_cpu = np.dot(data_cpu, data_cpu.T)  # æ…¢

# GPUç‰ˆæœ¬
data_gpu = cp.array(data_cpu)
result_gpu = cp.dot(data_gpu, data_gpu.T)  # å¿«10-100å€
```

### 2. åˆ†å¸ƒå¼è®¡ç®—ï¼ˆå¤§è§„æ¨¡ï¼‰

ä½¿ç”¨Daskè¿›è¡Œåˆ†å¸ƒå¼DataFrameæ“ä½œï¼š

```python
import dask.dataframe as dd

# è¯»å–å¤§æ–‡ä»¶
df = dd.read_csv('huge_data.csv')

# åˆ†å¸ƒå¼è®¡ç®—
result = df.groupby('symbol').mean().compute()
```

### 3. å®æ—¶æµå¤„ç†

ä½¿ç”¨å¼‚æ­¥I/Oå¤„ç†å®æ—¶æ•°æ®æµï¼š

```python
import asyncio
import aiohttp

async def stream_processor():
    async with aiohttp.ClientSession() as session:
        async with session.ws_connect('wss://stream.api.com') as ws:
            async for msg in ws:
                await process_message(msg)
```

---

## ğŸ“š å‚è€ƒèµ„æº

- [Numbaæ–‡æ¡£](https://numba.pydata.org/)
- [Pandasæ€§èƒ½ä¼˜åŒ–](https://pandas.pydata.org/docs/user_guide/enhancingperf.html)
- [NumPyæ€§èƒ½æŠ€å·§](https://numpy.org/doc/stable/user/performance.html)
- [Pythonæ€§èƒ½åˆ†æ](https://docs.python.org/3/library/profile.html)

---

**æœ€åæ›´æ–°**: 2025-10-06
**ä½œè€…**: Performance Optimization Team
